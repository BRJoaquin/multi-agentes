{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from games.kuhn import KuhnPoker\n",
    "from agents.counterfactualregret import CounterFactualRegret\n",
    "from agents.mcts_t import MonteCarloTreeSearch\n",
    "from tqdm import tqdm\n",
    "\n",
    "game = KuhnPoker(render_mode='')\n",
    "\n",
    "game.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfr_agent = CounterFactualRegret(game=game, agent=game.agents[0])\n",
    "cfr_agent2 = CounterFactualRegret(game=game, agent=game.agents[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [cfr_agent, cfr_agent2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [02:39<00:00, 625.86it/s]\n"
     ]
    }
   ],
   "source": [
    "cfr_agent.train(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t agent_0\t [0.75786677 0.24213323]\n",
      "0b\t agent_1\t [9.99984874e-01 1.51263046e-05]\n",
      "0p\t agent_1\t [0.66245565 0.33754435]\n",
      "0pb\t agent_0\t [9.99984933e-01 1.50665943e-05]\n",
      "1\t agent_0\t [9.99445073e-01 5.54926779e-04]\n",
      "1b\t agent_1\t [0.6659819 0.3340181]\n",
      "1p\t agent_1\t [9.99917094e-01 8.29062406e-05]\n",
      "1pb\t agent_0\t [0.42853112 0.57146888]\n",
      "2\t agent_0\t [0.27468689 0.72531311]\n",
      "2b\t agent_1\t [8.88152052e-05 9.99911185e-01]\n",
      "2p\t agent_1\t [2.51643081e-04 9.99748357e-01]\n",
      "2pb\t agent_0\t [1.49552837e-05 9.99985045e-01]\n"
     ]
    }
   ],
   "source": [
    "for n in sorted(cfr_agent.node_dict.keys()): \n",
    "    print(n + '\\t', cfr_agent.node_dict[n].agent + '\\t', cfr_agent.node_dict[n].policy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [02:42<00:00, 617.19it/s]\n"
     ]
    }
   ],
   "source": [
    "cfr_agent2.train(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t agent_0\t [0.83606926 0.16393074]\n",
      "0b\t agent_1\t [9.99985116e-01 1.48844963e-05]\n",
      "0p\t agent_1\t [0.66894054 0.33105946]\n",
      "0pb\t agent_0\t [9.99984996e-01 1.50042012e-05]\n",
      "1\t agent_0\t [9.99177688e-01 8.22311884e-04]\n",
      "1b\t agent_1\t [0.66077589 0.33922411]\n",
      "1p\t agent_1\t [9.99833560e-01 1.66439703e-04]\n",
      "1pb\t agent_0\t [0.50877182 0.49122818]\n",
      "2\t agent_0\t [0.5214957 0.4785043]\n",
      "2b\t agent_1\t [1.49853144e-05 9.99985015e-01]\n",
      "2p\t agent_1\t [2.99706288e-05 9.99970029e-01]\n",
      "2pb\t agent_0\t [1.50566129e-05 9.99984943e-01]\n"
     ]
    }
   ],
   "source": [
    "for n in sorted(cfr_agent2.node_dict.keys()): \n",
    "    print(n + '\\t', cfr_agent2.node_dict[n].agent + '\\t', cfr_agent2.node_dict[n].policy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [01:26<00:00, 11571.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rewards: {'agent_0': -0.054871, 'agent_1': 0.054871}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cum_rewards = dict(map(lambda ag: (ag, 0.), game.agents))\n",
    "niter = 1000000\n",
    "for _ in tqdm(range(niter)):\n",
    "    game.reset()\n",
    "    turn = 0\n",
    "    while not game.done():\n",
    "        agent_i = game.agent_name_mapping[game.agent_selection]\n",
    "        agent = agents[agent_i]\n",
    "        a = agent.action()\n",
    "        game.step(action=a)\n",
    "        turn += 1\n",
    "    for ag in game.agents:\n",
    "        cum_rewards[ag] += game.rewards[ag]\n",
    "print('Average rewards:', dict(map(lambda ag: (ag, cum_rewards[ag]/niter), game.agents)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 8889/100000 [00:14<02:24, 632.13it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/k/Documents/facultad/multi-agentes/Alternating Games/CFR_Leduc.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/k/Documents/facultad/multi-agentes/Alternating%20Games/CFR_Leduc.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m agent\u001b[39m.\u001b[39mtrain(\u001b[39m100_000\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/facultad/multi-agentes/Alternating Games/agents/counterfactualregret.py:79\u001b[0m, in \u001b[0;36mCounterFactualRegret.train\u001b[0;34m(self, niter)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m, niter\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m):\n\u001b[1;32m     78\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(niter)):\n\u001b[0;32m---> 79\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfr()\n",
      "File \u001b[0;32m~/Documents/facultad/multi-agentes/Alternating Games/agents/counterfactualregret.py:86\u001b[0m, in \u001b[0;36mCounterFactualRegret.cfr\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m game\u001b[39m.\u001b[39mreset()\n\u001b[1;32m     85\u001b[0m probability \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones(game\u001b[39m.\u001b[39mnum_agents)\n\u001b[0;32m---> 86\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfr_rec(game\u001b[39m=\u001b[39mgame, agent\u001b[39m=\u001b[39magent, probability\u001b[39m=\u001b[39mprobability)\n",
      "File \u001b[0;32m~/Documents/facultad/multi-agentes/Alternating Games/agents/counterfactualregret.py:135\u001b[0m, in \u001b[0;36mCounterFactualRegret.cfr_rec\u001b[0;34m(self, game, agent, probability)\u001b[0m\n\u001b[1;32m    132\u001b[0m     game_clone \u001b[39m=\u001b[39m game\u001b[39m.\u001b[39mclone()\n\u001b[1;32m    133\u001b[0m     game_clone\u001b[39m.\u001b[39mstep(a)\n\u001b[0;32m--> 135\u001b[0m     utility[a] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfr_rec(\n\u001b[1;32m    136\u001b[0m         game\u001b[39m=\u001b[39mgame_clone, agent\u001b[39m=\u001b[39magent, probability\u001b[39m=\u001b[39mnode_probability\n\u001b[1;32m    137\u001b[0m     )\n\u001b[1;32m    139\u001b[0m node_utility \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(utility \u001b[39m*\u001b[39m node\u001b[39m.\u001b[39mpolicy())\n\u001b[1;32m    141\u001b[0m \u001b[39m# update node cumulative regrets using regret matching\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39m# we only update the regrets of the agent that is building the strategy\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/facultad/multi-agentes/Alternating Games/agents/counterfactualregret.py:132\u001b[0m, in \u001b[0;36mCounterFactualRegret.cfr_rec\u001b[0;34m(self, game, agent, probability)\u001b[0m\n\u001b[1;32m    130\u001b[0m node_probability[game\u001b[39m.\u001b[39magent_name_mapping[current_agent]] \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mpolicy()[a]\n\u001b[1;32m    131\u001b[0m \u001b[39m# play action a\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m game_clone \u001b[39m=\u001b[39m game\u001b[39m.\u001b[39mclone()\n\u001b[1;32m    133\u001b[0m game_clone\u001b[39m.\u001b[39mstep(a)\n\u001b[1;32m    135\u001b[0m utility[a] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfr_rec(\n\u001b[1;32m    136\u001b[0m     game\u001b[39m=\u001b[39mgame_clone, agent\u001b[39m=\u001b[39magent, probability\u001b[39m=\u001b[39mnode_probability\n\u001b[1;32m    137\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/facultad/multi-agentes/Alternating Games/base/game.py:40\u001b[0m, in \u001b[0;36mAlternatingGame.clone\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclone\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 40\u001b[0m     game \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(\u001b[39mself\u001b[39m)\n\u001b[1;32m     41\u001b[0m     \u001b[39mreturn\u001b[39;00m game\n",
      "File \u001b[0;32m~/anaconda3/envs/multi-agentes/lib/python3.11/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[39m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[39m=\u001b[39m _reconstruct(x, memo, \u001b[39m*\u001b[39mrv)\n\u001b[1;32m    174\u001b[0m \u001b[39m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m~/anaconda3/envs/multi-agentes/lib/python3.11/copy.py:265\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39mif\u001b[39;00m deep \u001b[39mand\u001b[39;00m args:\n\u001b[1;32m    264\u001b[0m     args \u001b[39m=\u001b[39m (deepcopy(arg, memo) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args)\n\u001b[0;32m--> 265\u001b[0m y \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs)\n\u001b[1;32m    266\u001b[0m \u001b[39mif\u001b[39;00m deep:\n\u001b[1;32m    267\u001b[0m     memo[\u001b[39mid\u001b[39m(x)] \u001b[39m=\u001b[39m y\n",
      "File \u001b[0;32m~/anaconda3/envs/multi-agentes/lib/python3.11/copy.py:264\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    262\u001b[0m deep \u001b[39m=\u001b[39m memo \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[39mif\u001b[39;00m deep \u001b[39mand\u001b[39;00m args:\n\u001b[0;32m--> 264\u001b[0m     args \u001b[39m=\u001b[39m (deepcopy(arg, memo) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args)\n\u001b[1;32m    265\u001b[0m y \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs)\n\u001b[1;32m    266\u001b[0m \u001b[39mif\u001b[39;00m deep:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.train(100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t agent_0\t [9.99901106e-01 9.88935834e-05]\n",
      "0b\t agent_1\t [9.99985224e-01 1.47762870e-05]\n",
      "0p\t agent_1\t [9.99736888e-01 2.63112050e-04]\n",
      "0pb\t agent_0\t [9.99985136e-01 1.48641417e-05]\n",
      "1\t agent_0\t [9.99689863e-01 3.10136961e-04]\n",
      "1b\t agent_1\t [0.41357264 0.58642736]\n",
      "1p\t agent_1\t [9.99955604e-01 4.43957735e-05]\n",
      "1pb\t agent_0\t [0.49411001 0.50588999]\n",
      "2\t agent_0\t [3.68356216e-04 9.99631644e-01]\n",
      "2b\t agent_1\t [1.49799269e-05 9.99985020e-01]\n",
      "2p\t agent_1\t [1.49799269e-05 9.99985020e-01]\n",
      "2pb\t agent_0\t [1.48205235e-05 9.99985179e-01]\n"
     ]
    }
   ],
   "source": [
    "for n in sorted(agent.node_dict.keys()): \n",
    "    print(n + '\\t', agent.node_dict[n].agent + '\\t', agent.node_dict[n].policy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
