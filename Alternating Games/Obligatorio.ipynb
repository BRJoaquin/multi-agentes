{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obligatorio Muliti-agentes: CFR y MCTS\n",
    "\n",
    "---\n",
    "## Índice\n",
    "\n",
    "1. [Introducción](##Introducción)\n",
    "2. [Objetivos del Trabajo](##Objetivos-del-Trabajo)\n",
    "3. [Setup](##Setup)\n",
    "   1. [Entorno de Ejecución](###Entorno-de-Ejecución)\n",
    "   2. [Manejo de Dependencias](###Manejo-de-Dependencias)\n",
    "   2. [Instalación de Dependencias](###Instalación-de-Dependencias)\n",
    "4. [Counterfactual Regret Minimization](##Counterfactual-Regret-Minimization)\n",
    "   1. [Introducción a CFR](###Introducción-a-CFR)\n",
    "   2. [Implementación de CFR](###Implementación-de-CFR)\n",
    "   3. [Experimentación con Kuhn Poker](###Experimentación-con-Kuhn-Poker)\n",
    "5. [Monte Carlo Tree Search](##Monte-Carlo-Tree-Search)\n",
    "   1. [Implementación de MCTS y Funciones de Evaluación](###Implementación-de-MCTS-y-Funciones-de-Evaluación)\n",
    "   2. [Experimentación con Leduc Poker](###Experimentación-con-Leduc-Poker)\n",
    "6. [Parte C - Investigación](##Parte-C---Investigación)\n",
    "   1. [Tarea de Investigación](###Tarea-de-Investigación)\n",
    "7. [Conclusiones](##Conclusiones)\n",
    "8. [Referencias](##Referencias)\n",
    "9. [Descargo de Responsabilidad](##Descargo-de-Responsabilidad)\n",
    "\n",
    "\n",
    "## Introducción\n",
    "\n",
    "En este trabajo, exploramos la implementación y experimentación de algoritmos de aprendizaje avanzados en el contexto de juegos alternados de múltiples jugadores. Nos enfocamos específicamente en dos algoritmos clave: CFR (Counterfactual Regret Minimization) y MCTS (Monte Carlo Tree Search), aplicados a variantes de póker. Nuestro objetivo es no solo implementar estos algoritmos sino también investigar su rendimiento y efectividad en diferentes escenarios de juego.\n",
    "\n",
    "\n",
    "## Objetivos del Trabajo\n",
    "\n",
    "El principal objetivo de este trabajo es profundizar en el entendimiento y aplicación práctica de los algoritmos CFR y MCTS en entornos de juego competitivos, así como explorar posibles extensiones y mejoras a estos métodos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entorno de Ejecución\n",
    "\n",
    "[PettingZoo](https://pettingzoo.farama.org/index.html) es una biblioteca de juegos multiagentes que proporciona una amplia variedad de entornos diseñados específicamente para la investigación en aprendizaje por refuerzo. Este entorno será la base de nuestros experimentos, permitiéndonos simular y analizar de manera efectiva las interacciones y estrategias de los agentes en diferentes juegos.\n",
    "\n",
    "#### Características Clave de PettingZoo\n",
    "\n",
    "- **Variedad de Juegos**: Ofrece una extensa colección de juegos, incluyendo clásicos y nuevos desafíos, lo que nos permite explorar una amplia gama de escenarios y dinámicas de juego.\n",
    "- **Compatibilidad con Aprendizaje por Refuerzo**: Diseñado para ser compatible con las técnicas y algoritmos de aprendizaje por refuerzo más comunes, facilitando la integración y experimentación.\n",
    "- **Entorno Multiagente**: Especialmente orientado a entornos multiagentes, lo que lo hace ideal para estudiar juegos alternados de múltiples jugadores como los que abordaremos.\n",
    "\n",
    "#### Implementación de los Juegos\n",
    "\n",
    "En nuestro trabajo, la catedra nos proporcionó una implementación de los juegos Kuhn Poker y Leduc Poker, que utilizaremos para nuestros experimentos. Estas implementaciones se basan en la biblioteca PettingZoo.\n",
    "\n",
    "1. **Kuhn Poker**: Un juego de póker simplificado que servirá como campo de prueba para el algoritmo CFR. Experimentaremos tanto con versiones de 2 como de 3 jugadores.\n",
    "2. **Leduc Poker**: Un juego de póker más complejo que será utilizado para probar y evaluar el algoritmo MCTS, así como las funciones de evaluación integradas.\n",
    "\n",
    "Estos juegos nos proporcionarán una plataforma sólida para implementar, probar y analizar los algoritmos de aprendizaje en un contexto competitivo y controlado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Manejo de Dependencias\n",
    "\n",
    "Para garantizar la consistencia y reproducibilidad de nuestros experimentos, utilizaremos [Conda](https://docs.conda.io/en/latest/), un sistema de gestión de paquetes y entornos, que nos permitirá crear un entorno aislado con todas las dependencias necesarias para nuestro proyecto.\n",
    "\n",
    "> **Nota**: Para instalar Conda, siga las instrucciones en la [documentación oficial](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html).\n",
    "\n",
    "\n",
    "#### Ventajas de Usar Conda\n",
    "\n",
    "- **Reproducibilidad**: Conda nos ayuda a garantizar que todos los experimentos se puedan reproducir en diferentes máquinas con las mismas versiones de paquetes y dependencias.\n",
    "- **Gestión de Dependencias**: Maneja fácilmente las dependencias y paquetes necesarios para el proyecto.\n",
    "- **Entornos Aislados**: Permite crear entornos aislados para evitar conflictos entre distintos proyectos.\n",
    "\n",
    "\n",
    "#### Archivo `environment.yml`\n",
    "\n",
    "En la raíz del proyecto, hemos incluido un archivo `environment.yml` que contiene todas las especificaciones necesarias para crear el entorno de Conda adecuado. Este archivo incluye:\n",
    "\n",
    "- Las versiones específicas de Python y las bibliotecas necesarias.\n",
    "- Dependencias adicionales requeridas para ejecutar los algoritmos de aprendizaje por refuerzo y los juegos en PettingZoo.\n",
    "\n",
    "#### Creación del Entorno con `environment.yml`\n",
    "\n",
    "Para crear el entorno utilizando este archivo, ejecute el siguiente comando en su terminal:\n",
    "\n",
    "```bash\n",
    "conda env create -f environment.yml\n",
    "```\n",
    "\n",
    "Una vez creado el entorno, puede activarlo usando:\n",
    "\n",
    "```bash\n",
    "conda activate multi-agentes\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalación de Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entornos\n",
    "import pettingzoo\n",
    "from base.game import AlternatingGame, AgentID\n",
    "\n",
    "# Agentes\n",
    "from agents.counterfactualregret import CounterFactualRegret\n",
    "from agents.counterfactualregretv2 import EnhancedCounterFactualRegret\n",
    "from agents.mcts_t import MonteCarloTreeSearch\n",
    "from agents.agent_random import RandomAgent\n",
    "\n",
    "# Juegos\n",
    "from games.kuhn import KuhnPoker\n",
    "from games.kuhn3 import KuhnPoker3\n",
    "from games.leduc import Leduc\n",
    "\n",
    "# Herramientas\n",
    "from tqdm import tqdm\n",
    "from base.utils import run\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semilla para Reproducibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 134\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counterfactual Regret Minimization\n",
    "\n",
    "### Introducción a CFR\n",
    "\n",
    "La técnica de Counterfactual Regret Minimization (CFR) representa un hito fundamental en el campo del aprendizaje por refuerzo, particularmente en juegos de información imperfecta como el póker. CFR es un algoritmo que itera sobre posibles estrategias para minimizar el arrepentimiento contrafactual, es decir, la diferencia entre la recompensa obtenida y la que se hubiera obtenido si se hubiera jugado de manera óptima en cada estado del juego.\n",
    "\n",
    "#### Fundamentos\n",
    "\n",
    "El CFR trabaja analizando las decisiones tomadas en cada punto de decisión de un juego y evaluando el \"arrepentimiento\" de no haber tomado otras alternativas. Esta evaluación se basa en la noción de arrepentimiento contrafactual, que es la diferencia entre el beneficio obtenido y el beneficio que se hubiera obtenido si se hubiera jugado la mejor estrategia posible en retrospectiva.\n",
    "\n",
    "#### Aplicaciones\n",
    "\n",
    "Este método ha demostrado ser particularmente poderoso en juegos de póker, donde la información es incompleta y las estrategias óptimas no son siempre claras. La implementación de CFR en juegos como Kuhn Poker y Leduc Poker nos permite explorar cómo este algoritmo puede adaptarse y aprender estrategias efectivas en entornos de decisión complejos y dinámicos.\n",
    "\n",
    "#### Objetivo de la Sección\n",
    "\n",
    "En esta sección, nos enfocaremos en la implementación del algoritmo CFR. Detallaremos su estructura, funcionamiento y aplicaremos el algoritmo a juegos de póker de múltiples jugadores, analizando cómo se adapta y evoluciona la estrategia del juego a lo largo de múltiples iteraciones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación de CFR\n",
    "\n",
    "En este proyecto, hemos implementado el algoritmo Counterfactual Regret Minimization (CFR) para experimentar en juegos de póker (aunque podrian ser usados en otros contextos). Esta implementación se centra en desarrollar y refinar estrategias de juego en entornos de información imperfecta. El código completo para esta implementación se puede encontrar en el archivo `/agents/counterfactualregret.py` del proyecto.\n",
    "\n",
    "#### Descripción de la Implementación\n",
    "\n",
    "La implementación de CFR en nuestro proyecto consta de varias partes clave:\n",
    "\n",
    "1. **Clase `Node`**: Representa un nodo en el árbol de juego. Cada nodo contiene información sobre el estado del juego (information set), el agente actual, la observación recibida y mantiene un registro del arrepentimiento acumulado y la política aprendida.\n",
    "\n",
    "2. **Métodos de Actualización y Estrategia**: Dentro de la clase `Node`, se implementan métodos para actualizar los valores de arrepentimiento contrafactual y para ajustar la estrategia actual basada en estos arrepentimientos.\n",
    "\n",
    "3. **Clase `CounterFactualRegret`**: Extiende la clase `Agent` (basado en `PettingZoo`) y se utiliza para representar un agente que emplea CFR. Contiene métodos para realizar acciones, entrenar utilizando CFR y realizar el algoritmo de CFR de manera recursiva.\n",
    "\n",
    "4. **Función `cfr_rec`**: Implementa la lógica del CFR de manera recursiva, calculando la utilidad de cada nodo y actualizando la estrategia y los arrepentimientos.\n",
    "\n",
    "#### Naturaleza Agnóstica al Juego de las Implementaciones de Agentes\n",
    "\n",
    "Es importante destacar un aspecto fundamental de nuestras implementaciones de agentes, incluido el agente que utiliza el algoritmo Counterfactual Regret Minimization (CFR). Estos agentes son **agnósticos al juego**, lo que significa que su diseño y funcionamiento no están limitados o definidos por las reglas o acciones específicas de un juego en particular, o por la cantidad de jugadores. En cambio, estos agentes se pueden utilizar en cualquier juego que cumpla con los requisitos básicos de la biblioteca PettingZoo, lo que los hace altamente adaptables y reutilizables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentación con Kuhn Poker\n",
    "\n",
    "[Kuhn Poker](https://en.wikipedia.org/wiki/Kuhn_poker) es un juego de póker simplificado que sirve como un modelo excelente para estudiar algoritmos de aprendizaje por refuerzo en juegos de información imperfecta. Este juego representa una versión reducida del póker tradicional, lo que lo convierte en un entorno ideal para experimentos y análisis en el campo de la teoría de juegos y la inteligencia artificial.\n",
    "\n",
    "\n",
    "![CFR](_assets/CFR-poker.png)\n",
    "\n",
    "#### Descripción del Kuhn Poker\n",
    "\n",
    "- **Jugadores**: El juego clásico involucra a 2 jugadores, aunque puede adaptarse para 3 jugadores.\n",
    "- **Baraja**: Se utiliza una baraja reducida, típicamente con tres cartas (por ejemplo, un As, un Rey y una Reina).\n",
    "- **Dinámica del Juego**: Cada jugador recibe una carta y tiene la opción de apostar (bet) o pasar (check). El juego tiene una estructura de apuestas limitada y presenta oportunidades para [farolear](https://es.wikipedia.org/wiki/Farol_(envite)) y realizar estrategias basadas en la información limitada.\n",
    "\n",
    "#### Experimentación Planeada\n",
    "\n",
    "En nuestro proyecto, experimentaremos con Kuhn Poker de la siguiente manera:\n",
    "\n",
    "1. **Con 2 Jugadores**: Inicialmente, nos enfocaremos en la versión clásica de Kuhn Poker con 2 jugadores. Aquí, aplicaremos y evaluaremos el algoritmo de Counterfactual Regret Minimization para desarrollar estrategias efectivas y analizar cómo los agentes aprenden y se adaptan a lo largo de múltiples juegos.\n",
    "\n",
    "2. **Expansión a 3 Jugadores**: Posteriormente, expandiremos nuestra experimentación para incluir una versión de 3 jugadores del juego. Esta variante presenta desafíos adicionales y complejidades, permitiéndonos explorar cómo los algoritmos se adaptan a un entorno de juego más dinámico y a la presencia de un jugador adicional.\n",
    "\n",
    "Estas experimentaciones nos permitirán obtener una comprensión más profunda de la efectividad del algoritmo CFR y cómo se puede adaptar a diferentes configuraciones de juego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kuhn Poker con 2 Jugadores\n",
    "\n",
    "#### CFR vs Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "khun2 = KuhnPoker(render_mode='')\n",
    "khun2.reset() # Reseteamos el juego\n",
    "\n",
    "# Creamos los agentes\n",
    "cfr_agent = CounterFactualRegret(game=khun2, agent=khun2.agents[0])\n",
    "random_agent = RandomAgent(game=khun2, agent=khun2.agents[1])\n",
    "\n",
    "agents = {\n",
    "    khun2.agents[0]: cfr_agent,\n",
    "    khun2.agents[1]: random_agent\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:16<00:00, 607.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# entrenamos\n",
    "KHUN2_TRAINING_STEPS = 10000\n",
    "cfr_agent.train(KHUN2_TRAINING_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t agent_0\t [0.69924624 0.30075376]\n",
      "0b\t agent_1\t [9.99851500e-01 1.48500149e-04]\n",
      "0p\t agent_1\t [0.66378784 0.33621216]\n",
      "0pb\t agent_0\t [9.99852464e-01 1.47536146e-04]\n",
      "1\t agent_0\t [0.99593846 0.00406154]\n",
      "1b\t agent_1\t [0.66067448 0.33932552]\n",
      "1p\t agent_1\t [0.99538319 0.00461681]\n",
      "1pb\t agent_0\t [0.37501183 0.62498817]\n",
      "2\t agent_0\t [0.08510005 0.91489995]\n",
      "2b\t agent_1\t [1.47623265e-04 9.99852377e-01]\n",
      "2p\t agent_1\t [1.47623265e-04 9.99852377e-01]\n",
      "2pb\t agent_0\t [1.50240385e-04 9.99849760e-01]\n"
     ]
    }
   ],
   "source": [
    "# Imrimimos la estrategia de nuestro agente CFR\n",
    "def print_strategy(agent):\n",
    "    for n in sorted(agent.node_dict.keys()): \n",
    "        print(n + '\\t', agent.node_dict[n].agent + '\\t', agent.node_dict[n].policy())\n",
    "\n",
    "print_strategy(cfr_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [01:33<00:00, 10727.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.154535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "KHUN2_GAME_ROUNDS = 10_00_000\n",
    "_, mean_reward = run(khun2, agents, KHUN2_GAME_ROUNDS)\n",
    "\n",
    "print(mean_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que la estrategia aprendida por el agente CFR es mucho más efectiva que la estrategia aleatoria ya que en promedio gana más fichas. Esto se debe a que el agente CFR aprende a jugar de manera óptima en cada estado del juego, mientras que el agente aleatorio simplemente toma decisiones aleatorias.\n",
    "\n",
    "> Cabe destacar que en este caso el agente CFR es siempre el primer jugador, por lo que tiene una [desventaja inicial](https://en.wikipedia.org/wiki/Kuhn_poker#Optimal_strategy). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CFR vs CFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfr_agent2 = CounterFactualRegret(game=khun2, agent=khun2.agents[1])\n",
    "\n",
    "agents = {\n",
    "    khun2.agents[0]: cfr_agent,\n",
    "    khun2.agents[1]: cfr_agent2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:16<00:00, 596.66it/s]\n"
     ]
    }
   ],
   "source": [
    "cfr_agent2.train(KHUN2_TRAINING_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent 1 strategy:\n",
      "\n",
      "0\t agent_0\t [0.69924624 0.30075376]\n",
      "0b\t agent_1\t [9.99851500e-01 1.48500149e-04]\n",
      "0p\t agent_1\t [0.66378784 0.33621216]\n",
      "0pb\t agent_0\t [9.99852464e-01 1.47536146e-04]\n",
      "1\t agent_0\t [0.99593846 0.00406154]\n",
      "1b\t agent_1\t [0.66067448 0.33932552]\n",
      "1p\t agent_1\t [0.99538319 0.00461681]\n",
      "1pb\t agent_0\t [0.37501183 0.62498817]\n",
      "2\t agent_0\t [0.08510005 0.91489995]\n",
      "2b\t agent_1\t [1.47623265e-04 9.99852377e-01]\n",
      "2p\t agent_1\t [1.47623265e-04 9.99852377e-01]\n",
      "2pb\t agent_0\t [1.50240385e-04 9.99849760e-01]\n",
      "\n",
      "Agent 2 strategy:\n",
      "\n",
      "0\t agent_0\t [0.88786281 0.11213719]\n",
      "0b\t agent_1\t [9.99851764e-01 1.48235992e-04]\n",
      "0p\t agent_1\t [0.62804647 0.37195353]\n",
      "0pb\t agent_0\t [9.99848714e-01 1.51285930e-04]\n",
      "1\t agent_0\t [9.99846861e-01 1.53139357e-04]\n",
      "1b\t agent_1\t [0.65861866 0.34138134]\n",
      "1p\t agent_1\t [0.99878308 0.00121692]\n",
      "1pb\t agent_0\t [0.54095134 0.45904866]\n",
      "2\t agent_0\t [0.65005519 0.34994481]\n",
      "2b\t agent_1\t [1.49566258e-04 9.99850434e-01]\n",
      "2p\t agent_1\t [2.99132516e-04 9.99700867e-01]\n",
      "2pb\t agent_0\t [1.45645208e-04 9.99854355e-01]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAgent 1 strategy:\\n\")\n",
    "print_strategy(cfr_agent)\n",
    "\n",
    "print(\"\\nAgent 2 strategy:\\n\")\n",
    "print_strategy(cfr_agent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [01:27<00:00, 11398.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.057136\n"
     ]
    }
   ],
   "source": [
    "_, mean_reward = run(khun2, agents, KHUN2_GAME_ROUNDS)\n",
    "\n",
    "print(mean_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos concluir que llegamos a un [Equilibrio de Nash](https://es.wikipedia.org/wiki/Equilibrio_de_Nash), donde el primer jugador 1 tiene una recompensa esperada de ~1/18:\n",
    "\n",
    "> \"The game has a mixed-strategy Nash equilibrium; when both players play equilibrium strategies, the first player should expect to lose at a rate of −1/18 per hand (as the game is zero-sum, the second player should expect to win at a rate of +1/18). There is no pure-strategy equilibrium.\" - [Wikipedia](https://en.wikipedia.org/wiki/Kuhn_poker#Optimal_strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kuhn Poker con 3 Jugadores\n",
    "\n",
    "En esta sección, extendemos nuestra exploración del Kuhn Poker a un escenario más complejo y desafiante: un juego con tres jugadores. El Kuhn Poker de tres jugadores introduce una dinámica de juego adicional y requiere estrategias más sofisticadas, dada la presencia de un jugador extra. Este entorno nos permite investigar cómo los algoritmos de aprendizaje por refuerzo, específicamente el CFR, se adaptan y se desempeñan en un contexto de juego más complejo.\n",
    "\n",
    "#### CFR vs CFR vs CFR\n",
    "\n",
    "Para evaluar el rendimiento del algoritmo CFR en este contexto, configuramos un experimento donde tres agentes independientes, cada uno utilizando una instancia del algoritmo CFR, compiten entre sí. \n",
    "\n",
    "#### Expectativas de Utilidad: \\( u_1 < u_2 < u_3 \\)\n",
    "\n",
    "En términos de resultados, esperamos observar una relación específica en las utilidades obtenidas por los tres agentes, específicamente \\( u_1 < u_2 < u_3 \\). Esta expectativa se basa en la hipótesis de que el tercer jugador, al ser el último en actuar en cada ronda, tiene una ventaja estratégica al contar con más información que los otros dos jugadores. Del mismo modo, el segundo jugador tendría una ventaja sobre el primero. Esta hipótesis será probada y analizada a través de los resultados obtenidos en nuestras experimentaciones.\n",
    "\n",
    "Esta fase del proyecto no solo nos proporcionará insights sobre el rendimiento del CFR en un escenario de tres jugadores, sino que también ampliará nuestra comprensión sobre la dinámica de los juegos de información imperfecta y las estrategias óptimas en dichos contextos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "khun3 = KuhnPoker3(render_mode='')\n",
    "khun3.reset() # Reseteamos el juego\n",
    "\n",
    "# Creamos los agentes\n",
    "cfr_agent1 = CounterFactualRegret(game=khun3, agent=khun3.agents[0])\n",
    "cfr_agent2 = CounterFactualRegret(game=khun3, agent=khun3.agents[1])\n",
    "cfr_agent3 = CounterFactualRegret(game=khun3, agent=khun3.agents[2])\n",
    "\n",
    "\n",
    "agents = {\n",
    "    khun3.agents[0]: cfr_agent1,\n",
    "    khun3.agents[1]: cfr_agent2,\n",
    "    khun3.agents[2]: cfr_agent3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [06:40<00:00, 124.96it/s]\n",
      "100%|██████████| 50000/50000 [06:23<00:00, 130.41it/s]\n",
      "100%|██████████| 50000/50000 [06:23<00:00, 130.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# entrenamos\n",
    "KHUN3_TRAINING_STEPS = 50_000\n",
    "cfr_agent1.train(KHUN3_TRAINING_STEPS)\n",
    "cfr_agent2.train(KHUN3_TRAINING_STEPS)\n",
    "cfr_agent3.train(KHUN3_TRAINING_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent 1 strategy:\n",
      "\n",
      "0\t agent_0\t [0.99795265 0.00204735]\n",
      "0b\t agent_1\t [9.99976895e-01 2.31053604e-05]\n",
      "0bb\t agent_2\t [9.99976865e-01 2.31352952e-05]\n",
      "0bp\t agent_2\t [9.99976865e-01 2.31352952e-05]\n",
      "0p\t agent_1\t [0.9307425 0.0692575]\n",
      "0pb\t agent_2\t [9.99976865e-01 2.31352952e-05]\n",
      "0pbb\t agent_0\t [9.9997680e-01 2.3199703e-05]\n",
      "0pbp\t agent_0\t [9.99930401e-01 6.95991091e-05]\n",
      "0pp\t agent_2\t [0.82512136 0.17487864]\n",
      "0ppb\t agent_0\t [9.99860802e-01 1.39198218e-04]\n",
      "0ppbb\t agent_1\t [9.99630314e-01 3.69685767e-04]\n",
      "0ppbp\t agent_1\t [9.99815157e-01 1.84842884e-04]\n",
      "1\t agent_0\t [0.99430878 0.00569122]\n",
      "1b\t agent_1\t [9.99873700e-01 1.26299962e-04]\n",
      "1bb\t agent_2\t [9.99977028e-01 2.29716071e-05]\n",
      "1bp\t agent_2\t [9.99977028e-01 2.29716071e-05]\n",
      "1p\t agent_1\t [0.77882365 0.22117635]\n",
      "1pb\t agent_2\t [0.99683408 0.00316592]\n",
      "1pbb\t agent_0\t [9.99977004e-01 2.29959067e-05]\n",
      "1pbp\t agent_0\t [0.98723015 0.01276985]\n",
      "1pp\t agent_2\t [0.69007284 0.30992716]\n",
      "1ppb\t agent_0\t [0.99868436 0.00131564]\n",
      "1ppbb\t agent_1\t [9.99976898e-01 2.31021577e-05]\n",
      "1ppbp\t agent_1\t [9.99976898e-01 2.31021577e-05]\n",
      "2\t agent_0\t [9.99552745e-01 4.47255171e-04]\n",
      "2b\t agent_1\t [0.97247261 0.02752739]\n",
      "2bb\t agent_2\t [0.98687958 0.01312042]\n",
      "2bp\t agent_2\t [0.45797519 0.54202481]\n",
      "2p\t agent_1\t [0.99865718 0.00134282]\n",
      "2pb\t agent_2\t [0.97065754 0.02934246]\n",
      "2pbb\t agent_0\t [9.99771376e-01 2.28623685e-04]\n",
      "2pbp\t agent_0\t [0.52835058 0.47164942]\n",
      "2pp\t agent_2\t [9.99780546e-01 2.19453939e-04]\n",
      "2ppb\t agent_0\t [0.93099675 0.06900325]\n",
      "2ppbb\t agent_1\t [0.99818712 0.00181288]\n",
      "2ppbp\t agent_1\t [0.28695651 0.71304349]\n",
      "3\t agent_0\t [0.97839681 0.02160319]\n",
      "3b\t agent_1\t [2.29357798e-05 9.99977064e-01]\n",
      "3bb\t agent_2\t [1.85125191e-04 9.99814875e-01]\n",
      "3bp\t agent_2\t [9.25625955e-05 9.99907437e-01]\n",
      "3p\t agent_1\t [0.4262925 0.5737075]\n",
      "3pb\t agent_2\t [2.31406489e-05 9.99976859e-01]\n",
      "3pbb\t agent_0\t [1.86193735e-04 9.99813806e-01]\n",
      "3pbp\t agent_0\t [2.32742168e-05 9.99976726e-01]\n",
      "3pp\t agent_2\t [2.31406489e-05 9.99976859e-01]\n",
      "3ppb\t agent_0\t [2.32742168e-05 9.99976726e-01]\n",
      "3ppbb\t agent_1\t [2.29357798e-05 9.99977064e-01]\n",
      "3ppbp\t agent_1\t [2.29357798e-05 9.99977064e-01]\n",
      "\n",
      "Agent 2 strategy:\n",
      "\n",
      "0\t agent_0\t [9.99960276e-01 3.97235243e-05]\n",
      "0b\t agent_1\t [9.99959965e-01 4.00352310e-05]\n",
      "0bb\t agent_2\t [9.99959658e-01 4.03421010e-05]\n",
      "0bp\t agent_2\t [9.99959658e-01 4.03421010e-05]\n",
      "0p\t agent_1\t [0.90123857 0.09876143]\n",
      "0pb\t agent_2\t [9.99878974e-01 1.21026303e-04]\n",
      "0pbb\t agent_0\t [9.99960276e-01 3.97235243e-05]\n",
      "0pbp\t agent_0\t [9.99920553e-01 7.94470485e-05]\n",
      "0pp\t agent_2\t [0.57483791 0.42516209]\n",
      "0ppb\t agent_0\t [9.99841106e-01 1.58894097e-04]\n",
      "0ppbb\t agent_1\t [9.99839859e-01 1.60140924e-04]\n",
      "0ppbp\t agent_1\t [9.99839859e-01 1.60140924e-04]\n",
      "1\t agent_0\t [0.99649549 0.00350451]\n",
      "1b\t agent_1\t [9.99570910e-01 4.29089946e-04]\n",
      "1bb\t agent_2\t [9.99959629e-01 4.03714170e-05]\n",
      "1bp\t agent_2\t [9.99838514e-01 1.61485668e-04]\n",
      "1p\t agent_1\t [0.93070976 0.06929024]\n",
      "1pb\t agent_2\t [9.99738078e-01 2.61921876e-04]\n",
      "1pbb\t agent_0\t [9.99959586e-01 4.04138377e-05]\n",
      "1pbp\t agent_0\t [9.99515034e-01 4.84966052e-04]\n",
      "1pp\t agent_2\t [0.95286012 0.04713988]\n",
      "1ppb\t agent_0\t [9.99959586e-01 4.04138377e-05]\n",
      "1ppbb\t agent_1\t [9.99960128e-01 3.98724083e-05]\n",
      "1ppbp\t agent_1\t [0.99737969 0.00262031]\n",
      "2\t agent_0\t [9.99651883e-01 3.48116828e-04]\n",
      "2b\t agent_1\t [9.99102889e-01 8.97110939e-04]\n",
      "2bb\t agent_2\t [9.99689594e-01 3.10406469e-04]\n",
      "2bp\t agent_2\t [0.45218886 0.54781114]\n",
      "2p\t agent_1\t [0.99897061 0.00102939]\n",
      "2pb\t agent_2\t [0.99308189 0.00691811]\n",
      "2pbb\t agent_0\t [9.99959080e-01 4.09198789e-05]\n",
      "2pbp\t agent_0\t [0.48056626 0.51943374]\n",
      "2pp\t agent_2\t [9.9976294e-01 2.3706045e-04]\n",
      "2ppb\t agent_0\t [0.91587579 0.08412421]\n",
      "2ppbb\t agent_1\t [0.99707873 0.00292127]\n",
      "2ppbp\t agent_1\t [0.41226465 0.58773535]\n",
      "3\t agent_0\t [0.99176621 0.00823379]\n",
      "3b\t agent_1\t [3.99456739e-05 9.99960054e-01]\n",
      "3bb\t agent_2\t [3.97772474e-05 9.99960223e-01]\n",
      "3bp\t agent_2\t [1.19331742e-04 9.99880668e-01]\n",
      "3p\t agent_1\t [0.65828517 0.34171483]\n",
      "3pb\t agent_2\t [3.97772474e-05 9.99960223e-01]\n",
      "3pbb\t agent_0\t [3.89833151e-05 9.99961017e-01]\n",
      "3pbp\t agent_0\t [3.89833151e-05 9.99961017e-01]\n",
      "3pp\t agent_2\t [2.38663484e-04 9.99761337e-01]\n",
      "3ppb\t agent_0\t [1.55933261e-04 9.99844067e-01]\n",
      "3ppbb\t agent_1\t [3.99456739e-05 9.99960054e-01]\n",
      "3ppbp\t agent_1\t [3.99456739e-05 9.99960054e-01]\n",
      "\n",
      "Agent 3 strategy:\n",
      "\n",
      "0\t agent_0\t [0.99477446 0.00522554]\n",
      "0b\t agent_1\t [9.99959971e-01 4.00288208e-05]\n",
      "0bb\t agent_2\t [9.99959720e-01 4.02803512e-05]\n",
      "0bp\t agent_2\t [9.99959720e-01 4.02803512e-05]\n",
      "0p\t agent_1\t [0.75023663 0.24976337]\n",
      "0pb\t agent_2\t [9.99919439e-01 8.05607025e-05]\n",
      "0pbb\t agent_0\t [9.99959929e-01 4.00705241e-05]\n",
      "0pbp\t agent_0\t [9.99959929e-01 4.00705241e-05]\n",
      "0pp\t agent_2\t [0.57988883 0.42011117]\n",
      "0ppb\t agent_0\t [9.99959929e-01 4.00705241e-05]\n",
      "0ppbb\t agent_1\t [9.99919942e-01 8.00576415e-05]\n",
      "0ppbp\t agent_1\t [9.99919942e-01 8.00576415e-05]\n",
      "1\t agent_0\t [0.99060564 0.00939436]\n",
      "1b\t agent_1\t [9.99959830e-01 4.01703222e-05]\n",
      "1bb\t agent_2\t [9.99879256e-01 1.20743782e-04]\n",
      "1bp\t agent_2\t [0.99836749 0.00163251]\n",
      "1p\t agent_1\t [0.96461603 0.03538397]\n",
      "1pb\t agent_2\t [9.99919504e-01 8.04958545e-05]\n",
      "1pbb\t agent_0\t [9.99959726e-01 4.02738623e-05]\n",
      "1pbp\t agent_0\t [0.98484445 0.01515555]\n",
      "1pp\t agent_2\t [0.92585543 0.07414457]\n",
      "1ppb\t agent_0\t [9.99919452e-01 8.05477245e-05]\n",
      "1ppbb\t agent_1\t [9.99839319e-01 1.60681289e-04]\n",
      "1ppbp\t agent_1\t [9.99919659e-01 8.03406443e-05]\n",
      "2\t agent_0\t [9.99729441e-01 2.70559275e-04]\n",
      "2b\t agent_1\t [0.96951915 0.03048085]\n",
      "2bb\t agent_2\t [9.99722156e-01 2.77843931e-04]\n",
      "2bp\t agent_2\t [0.44954439 0.55045561]\n",
      "2p\t agent_1\t [0.99785127 0.00214873]\n",
      "2pb\t agent_2\t [0.99822364 0.00177636]\n",
      "2pbb\t agent_0\t [9.99960016e-01 3.99840064e-05]\n",
      "2pbp\t agent_0\t [0.4814413 0.5185587]\n",
      "2pp\t agent_2\t [0.99878485 0.00121515]\n",
      "2ppb\t agent_0\t [0.82066886 0.17933114]\n",
      "2ppbb\t agent_1\t [9.99474295e-01 5.25704572e-04]\n",
      "2ppbp\t agent_1\t [0.35867514 0.64132486]\n",
      "3\t agent_0\t [0.97555681 0.02444319]\n",
      "3b\t agent_1\t [3.98406375e-05 9.99960159e-01]\n",
      "3bb\t agent_2\t [1.59096333e-04 9.99840904e-01]\n",
      "3bp\t agent_2\t [3.97740832e-05 9.99960226e-01]\n",
      "3p\t agent_1\t [0.37645775 0.62354225]\n",
      "3pb\t agent_2\t [7.95481664e-05 9.99920452e-01]\n",
      "3pbb\t agent_0\t [0.00277646 0.99722354]\n",
      "3pbp\t agent_0\t [1.18990957e-04 9.99881009e-01]\n",
      "3pp\t agent_2\t [7.95481664e-05 9.99920452e-01]\n",
      "3ppb\t agent_0\t [3.96636522e-05 9.99960336e-01]\n",
      "3ppbb\t agent_1\t [3.98406375e-05 9.99960159e-01]\n",
      "3ppbp\t agent_1\t [3.98406375e-05 9.99960159e-01]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAgent 1 strategy:\\n\")\n",
    "print_strategy(cfr_agent1)\n",
    "\n",
    "print(\"\\nAgent 2 strategy:\\n\")\n",
    "print_strategy(cfr_agent2)\n",
    "\n",
    "print(\"\\nAgent 3 strategy:\\n\")\n",
    "print_strategy(cfr_agent3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000000/5000000 [07:58<00:00, 10441.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rewards: {'agent_0': -0.0260434, 'agent_1': -0.019646, 'agent_2': 0.0456894}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "KHUN3_GAME_ROUNDS = 5_000_000\n",
    "\n",
    "cum_rewards = dict(map(lambda ag: (ag, 0.), khun3.agents))\n",
    "\n",
    "for _ in tqdm(range(KHUN3_GAME_ROUNDS)):\n",
    "    khun3.reset()\n",
    "    turn = 0\n",
    "    while not khun3.done():\n",
    "        agent = agents[khun3.agent_selection]\n",
    "        a = agent.action()\n",
    "        khun3.step(action=a)\n",
    "        turn += 1\n",
    "    for ag in khun3.agents:\n",
    "        cum_rewards[ag] += khun3.rewards[ag]\n",
    "\n",
    "print('Average rewards:', dict(map(lambda ag: (ag, cum_rewards[ag]/KHUN3_GAME_ROUNDS), khun3.agents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, los resultados obtenidos en esta experimentación confirman nuestra hipótesis inicial: el tercer jugador obtiene la mayor utilidad, seguido del segundo jugador y finalmente el primer jugador.\n",
    "\n",
    "> \"A family of Nash equilibria for 3-player Kuhn poker is known analytically, which makes it the largest game with more than two players with analytic solution. The family is parameterized using 4–6 parameters (depending on the chosen equilibrium). In all equilibria, player 1 has a fixed strategy, and he always checks as the first action; player 2's utility is constant, equal to –1/48 per hand. The discovered equilibrium profiles show an interesting feature: by adjusting a strategy parameter $\\beta$ (between 0 and 1), player 2 can freely shift utility between the other two players while still remaining in equilibrium; player 1's utility is equal to $-\\frac{1+2\\beta}{48}$ (which is always worse than player 2's utility), player 3's utility is $\\frac{1+\\beta}{24}$.\" - [Wikipedia](https://en.wikipedia.org/wiki/Kuhn_poker#3-player_Kuhn_Poker)\n",
    "\n",
    "En nuestro caso no tenemos un $\\beta$ para parametrizar la estrategia del jugador 2, pero podemos ver que el jugador 2 tiene una utilidad similar a ~-1/48 por mano, que el jugador 3 tiene un poco mayor a 1/24 por mano, y que el jugador 1 tiene una utilidad un poco menor a -1/48 por mano. Lo cual es consistente con lo que se describe en trabajos previos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.9982\n"
     ]
    }
   ],
   "source": [
    "khun2 = KuhnPoker(render_mode='')\n",
    "khun2.reset() # Reseteamos el juego\n",
    "print(khun2.observe(khun2.agents[0]))\n",
    "mcts_estimador = MonteCarloTreeSearch(khun2, khun2.agents[0], 100, 100)\n",
    "v = mcts_estimador.estimate()\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "khun2 = KuhnPoker(render_mode='')\n",
    "khun2.reset() # Reseteamos el juego\n",
    "\n",
    "\n",
    "def estimator(game: AlternatingGame, agent: AgentID):\n",
    "    mcts_estimador = MonteCarloTreeSearch(game, agent)\n",
    "    v = mcts_estimador.estimate()\n",
    "    return v\n",
    "\n",
    "# Creamos los agentes\n",
    "cfr_agent = EnhancedCounterFactualRegret(game=khun2, agent=khun2.agents[0], value_estimator=estimator, max_depth=1)\n",
    "random_agent = RandomAgent(game=khun2, agent=khun2.agents[1])\n",
    "\n",
    "agents = {\n",
    "    khun2.agents[0]: cfr_agent,\n",
    "    khun2.agents[1]: random_agent\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:15<00:00,  3.24it/s]\n"
     ]
    }
   ],
   "source": [
    "cfr_agent.train(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t agent_0\t [0.03333333 0.96666667]\n",
      "1\t agent_0\t [0.02631579 0.97368421]\n",
      "2\t agent_0\t [0.02631579 0.97368421]\n"
     ]
    }
   ],
   "source": [
    "# Imrimimos la estrategia de nuestro agente CFR\n",
    "def print_strategy(agent):\n",
    "    for n in sorted(agent.node_dict.keys()): \n",
    "        print(n + '\\t', agent.node_dict[n].agent + '\\t', agent.node_dict[n].policy())\n",
    "\n",
    "print_strategy(cfr_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "- [PettingZoo: Gym for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2009.14471)\n",
    "- [Regret Circuits: Composability of Regret Minimizers](https://blog.ml.cmu.edu/2019/08/02/regret-circuits-composability-of-regret-minimizers/)\n",
    "- [Khun Poker](https://en.wikipedia.org/wiki/Kuhn_poker)\n",
    "- [Leduc Poker](https://en.wikipedia.org/wiki/Leduc_poker)\n",
    "- [Equilibrio de Nash](https://es.wikipedia.org/wiki/Equilibrio_de_Nash)\n",
    "- [Farol (envite)](https://es.wikipedia.org/wiki/Farol_(envite))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descargo de Responsabilidad\n",
    "\n",
    "### Participación de la Inteligencia Artificial en la Composición del Texto\n",
    "\n",
    "Este documento incluye secciones cuya composición textual ha sido asistida por una Inteligencia Artificial (IA). Es importante destacar que, aunque la IA ha contribuido en la redacción de ciertas partes del texto para mejorar la claridad y estructura del mismo, los siguientes elementos son enteramente responsabilidad del alumno:\n",
    "\n",
    "- **Desarrollo del Código**: Todo el código presente en este trabajo ha sido desarrollado y escrito por el alumno, reflejando su comprensión y aplicación práctica de los conceptos aprendidos.\n",
    "- **Conclusiones**: Las conclusiones extraídas de la experimentación y análisis son fruto del criterio y razonamiento del alumno, basadas en los resultados obtenidos y las observaciones realizadas durante el desarrollo del proyecto.\n",
    "- **Decisiones Metodológicas y Conceptuales**: Todas las decisiones relacionadas con la metodología, enfoque del proyecto, y la interpretación de los conceptos teóricos son producto del trabajo independiente del alumno.\n",
    "\n",
    "### Propósito de la Asistencia de IA\n",
    "\n",
    "El uso de la IA se ha limitado a proporcionar asistencia en la redacción para facilitar una comunicación clara y efectiva. En ningún momento, la IA ha influido en las decisiones técnicas, analíticas o conceptuales que conforman la esencia y los resultados del proyecto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
